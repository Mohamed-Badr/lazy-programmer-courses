{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural-Machine- Translation-seq2seq.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KVxJ8D4VS4vx"
      },
      "outputs": [],
      "source": [
        "import os \n",
        "import sys\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.optimizers import Adam, SGD"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# some config\n",
        "BATCH_SIZE = 64  # Batch size for training.\n",
        "EPOCHS = 40  # Number of epochs to train for.\n",
        "LATENT_DIM = 256  # Latent dimensionality of the encoding space.\n",
        "NUM_SAMPLES = 10000  # Number of samples to train on.\n",
        "MAX_NUM_WORDS = 20000\n",
        "EMBEDDING_DIM = 100"
      ],
      "metadata": {
        "id": "6smBezUlfj_o"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to store the data\n",
        "input_texts = []            # sentence in original language \n",
        "target_texts = []           # sentence in target language \n",
        "target_texts_inputs = []    # sentence in target language offset by 1"
      ],
      "metadata": {
        "id": "0pGzpRoogAVJ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load in the data\n",
        "# download the data at: http://www.manythings.org/anki/\n",
        "t = 0 \n",
        "for line in open('/content/spa.txt'):\n",
        "  #  only keep a limited number of samples\n",
        "  t += 1\n",
        "  if t > NUM_SAMPLES:\n",
        "    break\n",
        "\n",
        "  # input and target are separated by \\t\n",
        "  if '\\t' not in line:\n",
        "    continue\n",
        "\n",
        "  # split up the input translation\n",
        "  input_text, translation, garbage = line.split('\\t')\n",
        "\n",
        "  # make the target input and output\n",
        "  # recall we'll be using teacher forcing\n",
        "  target_text = translation + '<eos>'\n",
        "  target_text_input = '<sos>' + translation\n",
        "\n",
        "  input_texts.append(input_text)\n",
        "  target_texts.append(target_text)\n",
        "  target_texts_inputs.append(target_text_input)\n",
        "  "
      ],
      "metadata": {
        "id": "lhNyXyNcgfMn"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('num samples:', len(input_texts))\n",
        "print('first 10 input texts:', input_texts[:10])\n",
        "print('first 10 target texts:', target_texts[:10])\n",
        "print('first 10 target text inputs:', target_texts_inputs[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptXmunuQGNk5",
        "outputId": "fe5b21f8-dcdf-486d-886e-f4abfe9ba220"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num samples: 10000\n",
            "first 10 input texts: ['Go.', 'Go.', 'Go.', 'Go.', 'Hi.', 'Run!', 'Run!', 'Run!', 'Run!', 'Run.']\n",
            "first 10 target texts: ['Ve.<eos>', 'Vete.<eos>', 'Vaya.<eos>', 'Váyase.<eos>', 'Hola.<eos>', '¡Corre!<eos>', '¡Corran!<eos>', '¡Corra!<eos>', '¡Corred!<eos>', 'Corred.<eos>']\n",
            "first 10 target text inputs: ['<sos>Ve.', '<sos>Vete.', '<sos>Vaya.', '<sos>Váyase.', '<sos>Hola.', '<sos>¡Corre!', '<sos>¡Corran!', '<sos>¡Corra!', '<sos>¡Corred!', '<sos>Corred.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize the inputs\n",
        "tokenizer_inputs = Tokenizer(num_words=MAX_NUM_WORDS)\n",
        "tokenizer_inputs.fit_on_texts(input_texts)\n",
        "input_sequences = tokenizer_inputs.texts_to_sequences(input_texts)\n",
        "\n",
        "# tokenize the outputs\n",
        "# don't filter out spicial characters\n",
        "# otherwise <sos>, <eos> won't appear\n",
        "tokenizer_outputs = Tokenizer(num_words=MAX_NUM_WORDS, filters='')\n",
        "tokenizer_outputs.fit_on_texts(target_texts + target_texts_inputs) # ineficient\n",
        "target_sequences = tokenizer_outputs.texts_to_sequences(target_texts)\n",
        "target_sequences_inputs = tokenizer_outputs.texts_to_sequences(target_texts_inputs)"
      ],
      "metadata": {
        "id": "wClRf0kZHH1B"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get the word to index mapping for input language\n",
        "word2idx_inputs = tokenizer_inputs.word_index\n",
        "print('found %s unique input tokens.' % len(word2idx_inputs))\n",
        "\n",
        "# get the word to index mapping for output language\n",
        "word2idx_outputs = tokenizer_outputs.word_index\n",
        "print('found %s unique output tokens.' % len(word2idx_outputs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "akIyx034ImPM",
        "outputId": "0675191a-0c60-4639-f39e-0daf78420396"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "found 2288 unique input tokens.\n",
            "found 12093 unique output tokens.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# determine maximum length input sequence\n",
        "max_len_input = max(len(s) for s in input_sequences)\n",
        "\n",
        "# determine maximum length output sequence\n",
        "max_len_target = max(len(s) for s in target_sequences)\n",
        "\n",
        "# store number of output words for latter\n",
        "# remember to add 1 since indexing starts from 1\n",
        "num_words_output =  len(word2idx_outputs) + 1"
      ],
      "metadata": {
        "id": "sY6_14vzK_N-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pad sequences so that we get a N * T matrix\n",
        "encoder_inputs = pad_sequences(input_sequences, maxlen=max_len_input)\n",
        "print('shape of encoder data tensor:', encoder_inputs.shape)\n",
        "print('encoder_data[0]:', encoder_inputs[0])\n",
        "\n",
        "decoder_inputs = pad_sequences(target_sequences_inputs, maxlen=max_len_target, padding='post')\n",
        "print('\\nshape of decoder data tensor:', decoder_inputs.shape)\n",
        "print('decoder_inputs[0]:', decoder_inputs[0])\n",
        "\n",
        "decoder_targets = pad_sequences(target_sequences, maxlen=max_len_target, padding='post')\n",
        "print('\\nshape of decoder data tensor:', decoder_targets.shape)\n",
        "print('decoder_inputs[0]:', decoder_targets[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIrCPIJ0MUpn",
        "outputId": "b507e622-f42f-4826-d172-c7a0f1cc975c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of encoder data tensor: (10000, 5)\n",
            "encoder_data[0]: [ 0  0  0  0 15]\n",
            "\n",
            "shape of decoder data tensor: (10000, 8)\n",
            "decoder_inputs[0]: [8520    0    0    0    0    0    0    0]\n",
            "\n",
            "shape of decoder data tensor: (10000, 8)\n",
            "decoder_inputs[0]: [2756    0    0    0    0    0    0    0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# store all pre-traind word vectors\n",
        "print('Loading word vector...')\n",
        "word2vec = {}\n",
        "with open('/content/glove.6B.%sd.txt' % EMBEDDING_DIM) as f:\n",
        "  # Is just a space-separted text file in the format:\n",
        "  # word vec[0] vec[1] vec[2] ...\n",
        "  for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    vec = np.asarray(values[1:], dtype='float32')\n",
        "    word2vec[word] = vec\n",
        "print('Found %s word vectors: ' % len(word2vec))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmbo-Uf9N72N",
        "outputId": "662b5d07-6ecd-4168-8b08-bbd1b110e161"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading word vector...\n",
            "Found 114621 word vectors: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare embedding matrix\n",
        "print('filling pre-trained embedding...')\n",
        "num_words = min(MAX_NUM_WORDS, len(word2idx_inputs) + 1 ) \n",
        "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))   # 3000 * 50 - V * D\n",
        "for word, i in word2idx_inputs.items():\n",
        "  if i < MAX_NUM_WORDS:\n",
        "    embedding_vector = word2vec.get(word)\n",
        "    if embedding_vector is not None:\n",
        "      # words not found in embedding index will be zeros.\n",
        "      embedding_matrix[i] = embedding_vector"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UM-rPxBZPQWa",
        "outputId": "00ca34c6-66a6-44fc-a866-e9ddd7fca369"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "filling pre-trained embedding...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load per-trained word embeddings into an Embedding layer \n",
        "# note that we set trainable = False so as it keep embedding fixed.\n",
        "embedding_layer = Embedding(\n",
        "    num_words,\n",
        "    EMBEDDING_DIM,\n",
        "    weights = [embedding_matrix],\n",
        "    input_length = max_len_input,\n",
        "    #trainable = False\n",
        ")"
      ],
      "metadata": {
        "id": "BZccgzbTPwbw"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create targets -> can't use sparse cross entropy\n",
        "# categorical cross entropy when we have sequences\n",
        "decoder_targets_one_hot = np.zeros((len(input_texts), max_len_target, num_words_output)) \n",
        "for i, target_seq in enumerate(decoder_targets):\n",
        "  for t, word in enumerate(target_seq):\n",
        "    if word > 0:\n",
        "      decoder_targets_one_hot[i, t, word] = 1"
      ],
      "metadata": {
        "id": "Y6id23FNQxyC"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Building model...')\n",
        "\n",
        "encoder_inputs_placeholder = Input(shape=(max_len_input,))\n",
        "x = embedding_layer(encoder_inputs_placeholder)\n",
        "encoder = LSTM(\n",
        "  LATENT_DIM,\n",
        "  return_state=True,\n",
        "  # dropout=0.5 # dropout not available on gpu\n",
        ")\n",
        "encoder_outputs, h, c = encoder(x)\n",
        "# encoder_outputs, h = encoder(x) #gru\n",
        "\n",
        "# keep only the states to pass into decoder\n",
        "encoder_states = [h, c]\n",
        "# encoder_states = [h]  # gru\n",
        "\n",
        "# set up the decoder, using [h, c] as inital state.\n",
        "decoder_inputs_placeholder = Input(shape=(max_len_target,))\n",
        "\n",
        "# this word embedding will not use pre-trained vectors\n",
        "# although you could\n",
        "decoder_embedding = Embedding(num_words_output, EMBEDDING_DIM)\n",
        "decoder_inputs_x = decoder_embedding( decoder_inputs_placeholder )\n",
        "\n",
        "# since the decoder is a \"to_many\" model we want to have\n",
        "# return_sequences = True\n",
        "decoder_lstm = LSTM(\n",
        "  LATENT_DIM,\n",
        "  return_sequences=True,\n",
        "  return_state=True,\n",
        "  # dropout=0.5 # dropout not available on gpu\n",
        ")\n",
        "decoder_outputs, _, _ = decoder_lstm(\n",
        "  decoder_inputs_x,\n",
        "  initial_state=encoder_states\n",
        ")\n",
        "# decoder_outputs, _ = decoder_gru(decoder_inputs_x, initial_state=encoder_states)\n",
        "\n",
        "# final dense layer for prediction\n",
        "decoder_dense = Dense(num_words_output, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "model = Model([encoder_inputs_placeholder, decoder_inputs_placeholder], decoder_outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWn7ADluRwkf",
        "outputId": "56f6fdbc-b12f-4c58-d630-68997efcb93f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building model...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "print('Training model...')\n",
        "h = model.fit(\n",
        "    [encoder_inputs, decoder_inputs],\n",
        "    decoder_targets_one_hot,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs= EPOCHS,\n",
        "    validation_split=0.2\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQ-HNVnEUWpr",
        "outputId": "37ce129c-0082-4527-c346-1c81959a1324"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(h.history['loss'] ,label='loss')\n",
        "plt.plot(h.history['val_loss'], label='val_loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gg3C8XrK8e5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(h.history['accuracy'] ,label='acc')\n",
        "plt.plot(h.history['val_accuracy'], label='val_acc')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uvM7Rgs48e8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##### Making Predictions #####\n",
        "# As with poetry example, we need to create another model\n",
        "# that can take in the RNN state and previous word as input\n",
        "# and accept a T=1 sequence.\n",
        "\n",
        "# The encoder will be stand-alone\n",
        "# from this we will get our initial decoder hidden state\n",
        "encoder_model = Model(encoder_inputs_placeholder, encoder_states)\n",
        "\n",
        "decoder_state_input_h = Input(shape=(LATENT_DIM,))\n",
        "decoder_state_input_c = Input(shape=(LATENT_DIM,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "#decoder_states_inputs = [decoder_state_input_h] # gru\n",
        "\n",
        "decoder_inputs_signal = Input(shape=(1,))\n",
        "decoder_inputs_signal_x = decoder_embbeding(decoder_inputs_signal)\n",
        "\n",
        "# this time, we want to keep the states to be the output\n",
        "# by out sampling model\n",
        "decoder_outputs, h, c = decoder_lstm( decoder_inputs_signal_x, initial_state=decoder_states_inputs)\n",
        "# decoder_outputs, h = decoder_lstm( decoder_inputs_signal_x, initial_state=decoder_states_inputs) # gru\n",
        "\n",
        "decoder_States = [h, c]\n",
        "#decoder_States = [h] # gru\n",
        "\n",
        "# the sampling model\n",
        "# inputs: y(t-1), h(t-1), c(t-1)\n",
        "# outputs: y(t), h(t), c(t)\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs_signal] + decoder_states_inputs,\n",
        "    [decoder_outputs] + decoder_States\n",
        ")"
      ],
      "metadata": {
        "id": "jgJ6krn2-k0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# map indexes back into real words\n",
        "# so we can view the results\n",
        "idx2word_eng = {v:k for k, v in word2idx_inputs.items()}\n",
        "idx2word_trans = {v:k for k, v in word2idx_outputs.items()}"
      ],
      "metadata": {
        "id": "rkszDKVh-k21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decoder_sequences(input_seq):\n",
        "  # encode the input as state vectors.\n",
        "  states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "  # generate empty target sequence of length 1.\n",
        "  target_seq = np.zeros((1, 1))\n",
        "\n",
        "  # populate the first character of target sequence with the start character.\n",
        "  # NOTE: tokenizer lower-cases all words\n",
        "  target_seq[0, 0] = word2idx_outputs['<sos>']\n",
        "\n",
        "  # if we get this we break\n",
        "  eos = word2idx_outputs['<eos>']\n",
        "\n",
        "  # create the translation\n",
        "  output_sentence = []\n",
        "  for _ in range(max_len_target):\n",
        "    output_tokens, h, c = decoder_model.predict(\n",
        "        [target_seq] + states_value\n",
        "    )\n",
        "    # output_tokens, h = decoder_model.predict( [target_seq] + states_value) # gru\n",
        "\n",
        "    # get next word\n",
        "    idx = np.argmax(output_tokens[0, 0, :])\n",
        "\n",
        "    # End sentence of EOS\n",
        "    if eos == idx:\n",
        "      break\n",
        "\n",
        "    word = ''\n",
        "    if idx > 0:\n",
        "      word = idx2word_trans[idx]\n",
        "      output_sentence.append(word)\n",
        "\n",
        "    # update the decoder input\n",
        "    # which is just the word just generated \n",
        "    target_seq[0, 0] = idx\n",
        "\n",
        "    # update states\n",
        "    states_value = [h, c]\n",
        "    # states_value = [h] # gru\n",
        "\n",
        "    return ' '.join(output_sentence)"
      ],
      "metadata": {
        "id": "lnaLEQKK-k5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "  # do some test translation\n",
        "  i = np.random.chice(  len(input_texts)  )\n",
        "  input_seq = encoder_inputs[i:i+1]\n",
        "  translation = decode_sequence(input_seq)\n",
        "  print('-')\n",
        "  print('Input:', input_texts[i])\n",
        "  print('translation:', translation)\n",
        "\n",
        "  ans = input('Continue? [Y/n]')\n",
        "  if ans and ans.lower().startswith('n'):\n",
        "    break"
      ],
      "metadata": {
        "id": "vcKkbwNE0tyC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}